{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUVuQm0mFjYocio81FffSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/webb-e/S2_Landsat_Comparison/blob/main/postprocessing_lakes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## google drive setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "## import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmtyPEGQR02S",
        "outputId": "501c58c6-3d23-44a3-c46f-39961c25e718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in data, filter, and put in wide format"
      ],
      "metadata": {
        "id": "wJo30BeWVqL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region = 'AKCP' ## options = AKCP, YKD, YKF, AND, TUK, MRD\n",
        "\n",
        "# Read the CSV files\n",
        "\n",
        "## csv file with lake area time series\n",
        "df_string = 'ALPOD/Lakewise_csvs/' + region + '_lake_areas.csv'\n",
        "df =  pd.read_csv(df_string)\n",
        "\n",
        "## csv file with region-specific factal dimensions\n",
        "fractaldf = pd.read_csv('ALPOD/fractal_dimensions.csv')\n",
        "\n",
        "## csv file with landsat time series\n",
        "landsat_string = 'ALPOD/Lakewise_csvs/Landsat_lake_areas_' + region + '.csv'\n",
        "landsat = pd.read_csv(landsat_string)\n",
        "\n",
        "## csv file with cloudiness\n",
        "cloud_string = 'ALPOD/Lakewise_csvs/Lakewise_cloudiness_' + region + '.csv'\n",
        "clouddf = pd.read_csv(cloud_string)"
      ],
      "metadata": {
        "id": "plV1zs2VYRVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Veby7Yn1P_Ns"
      },
      "outputs": [],
      "source": [
        "### get data in the right format\n",
        "\n",
        "# Create the 'year_week' column\n",
        "df['year_week'] = df['year'].astype(str) + df['week'].astype(str)\n",
        "\n",
        "# filter rows where area_km2 > 0.001 (the detection limit of Landsat)\n",
        "df_filtered = df[df['area_km2'] > 0.001]\n",
        "\n",
        "# convert from m2 to km2\n",
        "df_filtered['S2_water_km2'] = df_filtered['S2_water_m2'] / 1000000\n",
        "df_filtered = df_filtered.drop(['S2_water_m2'], axis=1)\n",
        "\n",
        "# transform the data to wide format\n",
        "dflt = df_filtered.pivot(index='year_week', columns='lake_id', values='S2_water_km2')\n",
        "dflt.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply percentile filter; everything in the top or lower 10% for each lake is deleted"
      ],
      "metadata": {
        "id": "w4_xY0FUgder"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def percentile_filter(x, lower_percentile=10, upper_percentile=90):\n",
        "    # Ensure that x is a Series\n",
        "    x = pd.Series(x)\n",
        "\n",
        "    # Calculate the percentiles\n",
        "    lower_bound = np.percentile(x.dropna(), lower_percentile)\n",
        "    upper_bound = np.percentile(x.dropna(), upper_percentile)\n",
        "\n",
        "    # Filter based on percentiles\n",
        "    filtered_data = x.copy()\n",
        "    filtered_data[(x < lower_bound) | (x > upper_bound)] = np.nan\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "# Apply the  percentile filter to each column\n",
        "filtered_df = dflt.apply(lambda x: percentile_filter(x, lower_percentile=10, upper_percentile=90), axis=0)\n",
        "\n",
        "df_percent = pd.DataFrame(data = filtered_df.values, index=dflt.index, columns=dflt.columns.values).reset_index()\n",
        "df_percent['year'] = df_percent['year_week'].str.slice(0, 4)\n",
        "df_percent.drop(['year_week'], axis=1, inplace=True)\n",
        "df_percent.head()"
      ],
      "metadata": {
        "id": "-tD2Kx3PIlpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the annual lake-wise surface water mean, min, max, and standard deviation."
      ],
      "metadata": {
        "id": "YccldwHJZ3ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats = df_percent.groupby('year').agg(['mean', 'min', 'max', 'std'])\n",
        "## change from muliti-level to the format we want; reset index to turn year into a column\n",
        "df_stacked = stats.stack(level=0).reset_index(name='lake_id')\n"
      ],
      "metadata": {
        "id": "8kTnouQ4ULxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate shoreline complexity; use equation form Seekell et al., 2022"
      ],
      "metadata": {
        "id": "q21lduwgaS2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## from the original dataframe, get columns we want to combine with the lake-wise stats\n",
        "picked_df = df_filtered.groupby('lake_id').first().reset_index()\n",
        "picked_df = picked_df.drop(['year_week', 'S2_water_km2', 'week', 'year'], axis=1)\n",
        "\n",
        "## merge with fractal df to get the region-wide fractal dimension\n",
        "df_merged = pd.merge(picked_df, fractaldf, left_on='region', right_on = 'Region', how='left')\n",
        "df_merged = df_merged.drop(['Region', 'Intercept', 'R-squared', 'P-value', 'Std Error'], axis=1)\n",
        "\n",
        "## calculate shoreline complexity\n",
        "denominator =  2 * math.sqrt(math.pi) * df_merged['area_km2']**(df_merged['Fractal Dimension']/2)\n",
        "df_merged['shoreline_complexity'] = df_merged['perim_km'] / denominator\n"
      ],
      "metadata": {
        "id": "gB8R-KswNIf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine with lake-wise properties from the original dataframe and with the Landsat dataframe\n"
      ],
      "metadata": {
        "id": "-6hT42IFaHs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### merge df with shoreline complexity and lake properties with df on lake stats\n",
        "df_fin  = pd.merge(df_stacked, df_merged, left_on='level_1', right_on = 'lake_id', how='left')\n",
        "df_fin  = df_fin.drop(['level_1', 'Fractal Dimension'], axis=1)\n",
        "\n",
        "## rename to names we want\n",
        "df_fin = df_fin.rename({'mean': 'S2mean', 'max': 'S2max','min': 'S2min','std': 'S2std'}, axis=1)\n",
        "\n",
        "## calculate coefficient of variation\n",
        "df_fin['S2cv']  = df_fin['S2std'] / df_fin['S2mean']\n",
        "\n",
        "## update year type for merge\n",
        "df_fin['year'] = df_fin['year'].astype(int)"
      ],
      "metadata": {
        "id": "15DwmaMMWpjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get landsat dataframe into shape and then combine with other data"
      ],
      "metadata": {
        "id": "zPpzyH-BrVRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### get landsat dataframe to not have repeated lake_id/year combinations\n",
        "landsat2 = landsat.groupby(['lake_id', 'year']).apply(lambda group: group.ffill().bfill()).drop_duplicates()\n",
        "landsat2.reset_index(drop=True, inplace=True)\n",
        "\n",
        "### convert from m2 to km2\n",
        "landsat2['Landsat_Pickens'] = landsat2['Landsat_Pickens']/1000000\n",
        "landsat2['Landsat_Pekel'] = landsat2['Landsat_Pekel']/1000000\n",
        "\n",
        "df_pluslandsat= df_fin.merge(landsat2, on=['lake_id', 'year'], how='left')\n"
      ],
      "metadata": {
        "id": "yk8sgfh8rUKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = df_pluslandsat.merge(clouddf, on=['lake_id', 'year'], how='left')\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "TqFxGK3erhAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## calculate error between S2 and Landsat\n",
        "final_df['Pickens_error_abs'] = final_df['Landsat_Pickens'] - final_df['S2max']\n",
        "final_df['Pekel_error_abs'] = final_df['Landsat_Pekel'] - final_df['S2max']\n",
        "\n",
        "final_df['Pickens_error_per'] = (final_df['Landsat_Pickens'] - final_df['S2max'])/final_df['S2max']\n",
        "final_df['Pekel_error_per'] =   (final_df['Landsat_Pekel'] - final_df['S2max'])/final_df['S2max']\n"
      ],
      "metadata": {
        "id": "RizsLKbXeFoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save to CSV!"
      ],
      "metadata": {
        "id": "wjuLMyD6dmnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_string = 'ALPOD/Lakewise_csvs/analysis_ready_' + region + '.csv'\n",
        "final_df.to_csv(export_string, index=False)"
      ],
      "metadata": {
        "id": "l2Nxrv3PRZeO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
