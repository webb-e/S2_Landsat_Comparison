{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/webb-e/S2_Landsat_Comparison/blob/main/Batch_LakeOccurrence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8Bxr0e8HM0Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fc49b754-f7dc-48bf-8b14-2b37c33d410c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nauthor: @ericslevenson, updated by @ElizabethWebb\\ndate: June 2024\\ndescription: Ingest S2 tiles, export weekly lake occurance map\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "author: @ericslevenson, updated by @ElizabethWebb\n",
        "date: June 2024\n",
        "description: Ingest S2 tiles, export weekly lake occurance map\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raErdXDCB-vO"
      },
      "outputs": [],
      "source": [
        "# Authenticate private account (only required for exporting to drive/gee/gcp)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Earth Engine setup\n",
        "import ee # Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the library. ## add your project ID here\n",
        "ee.Initialize(project=\" \")\n",
        "\n",
        "# Some common imports\n",
        "from IPython.display import Image\n",
        "import folium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regionfolder = None\n",
        "lakestring = None\n",
        "ids = None\n",
        "def regionfun(region):\n",
        "  global regionfolder, lakestring, ids\n",
        "  if region == 'TUK/MRD':\n",
        "    regionfolder = 'TUK_MRD_weekly/'\n",
        "    lakestring = \"projects/alpod-412314/assets/Lake_extractions/TUK_MRD_AND_extraction\"\n",
        "    ids = ['07WFS', '07WFT', '08WMA', '08WMB', '08WMC', '08WMV', '08WNA', '08WNB', '08WNC', '08WNV', '08WPB', '08WPC', '08WPD', '09WVT', '09WVU']\n",
        "  elif region == 'AKCP':\n",
        "    regionfolder = 'AKCP_weekly/'\n",
        "    lakestring = \"projects/alpod-412314/assets/Lake_extractions/AKCP\"\n",
        "    ids = ['03WWT', '03WXT', '03WXU', '04WDC', '04WDD', '04WEC', '04WED', '04WEE', '04WFC', '04WFD', '04WFE', '05WMT', '05WMU', '05WMV', '05WNT',\n",
        "         '05WNU', '05WPT','05WPU', '06WVC', '06WVD', '06WWC', '06WXC', '07WDT']\n",
        "  elif region == 'YKD':\n",
        "    regionfolder = 'YKD_weekly/'\n",
        "    lakestring = \"projects/alpod-412314/assets/Lake_extractions/YKD\"\n",
        "    ids = ['03VVH', '03VVJ', '03VVK', '03VWJ', '03VWG', '03VWH','03VWK', '03VWL', '03VXG', '03VXH', '03VXJ', '03VXK', '03VXL', '04VCM', '04VCN', '04VCP', '04VCQ',\n",
        "          '04VCR', '04VDM', '04VDN', '04VDP', '04VEP']\n",
        "  elif region == 'YKF':\n",
        "    regionfolder = 'YKF_weekly/'\n",
        "    lakestring = \"projects/alpod-412314/assets/Lake_extractions/YKF\"\n",
        "    ids = ['05WPP', '05WPQ', '06WVU', '06WVV', '06WWT', '06WWU', '06WWV', '06WXU', '06WXV', '07WDP', '07WDQ']\n",
        "\n",
        "  else:\n",
        "    print(\"Invalid region\")"
      ],
      "metadata": {
        "id": "yR5EpGLk1UGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nb_LpkAC_x3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa42311-8878-4991-f32b-e98ceb37897f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'type': 'Date', 'value': 1462060800000}, {'type': 'Date', 'value': 1462665600000}, {'type': 'Date', 'value': 1463270400000}, {'type': 'Date', 'value': 1463875200000}, {'type': 'Date', 'value': 1464480000000}, {'type': 'Date', 'value': 1465084800000}, {'type': 'Date', 'value': 1465689600000}, {'type': 'Date', 'value': 1466294400000}, {'type': 'Date', 'value': 1466899200000}, {'type': 'Date', 'value': 1467504000000}, {'type': 'Date', 'value': 1468108800000}, {'type': 'Date', 'value': 1468713600000}, {'type': 'Date', 'value': 1469318400000}, {'type': 'Date', 'value': 1469923200000}, {'type': 'Date', 'value': 1470528000000}, {'type': 'Date', 'value': 1471132800000}, {'type': 'Date', 'value': 1471737600000}, {'type': 'Date', 'value': 1472342400000}, {'type': 'Date', 'value': 1472947200000}, {'type': 'Date', 'value': 1473552000000}, {'type': 'Date', 'value': 1474156800000}, {'type': 'Date', 'value': 1474761600000}, {'type': 'Date', 'value': 1475366400000}]\n"
          ]
        }
      ],
      "source": [
        "## *** INPUTS THAT NEED TO BE ADJUSTED ***\n",
        "year = str(2016)\n",
        "regionfun('AKCP') # options = TUK/MRD, AKCP, YKD, YKF\n",
        "lakes = ee.FeatureCollection(lakestring)\n",
        "\n",
        "start1 = year +'-05-01'\n",
        "finish1 = year +'-09-30'\n",
        "\n",
        "# Tiled ROI\n",
        "tiles = ee.FeatureCollection('projects/ee-webbe/assets/ALPOD/S2_tiles')\n",
        "pixScale = 10\n",
        "eestart = ee.Date(start1)\n",
        "eefinish = ee.Date(finish1)\n",
        "\n",
        "### define weeks\n",
        "n_weeks = eefinish.difference(eestart,'week').round();\n",
        "dates = ee.List.sequence(0, n_weeks, 1);\n",
        "def make_datelist(n):\n",
        "      return eestart.advance(n,'week')\n",
        "\n",
        "dates = dates.map(make_datelist)\n",
        "\n",
        "\n",
        "dates_list = dates.getInfo()\n",
        "print(dates_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M17JcGSkEsUc"
      },
      "source": [
        "### Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk6QSMoVEtWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a572d5-16eb-42cb-ed83-1c726a37a48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image export 05WNU_2016_17 has started\n",
            "Image export 05WNU_2016_18 has started\n",
            "Image export 05WNU_2016_19 has started\n",
            "Image export 05WNU_2016_20 has started\n",
            "Image export 05WNU_2016_21 has started\n",
            "Image export 05WNU_2016_22 has started\n",
            "Image export 05WNU_2016_23 has started\n",
            "Image export 05WNU_2016_24 has started\n",
            "Image export 05WNU_2016_25 has started\n",
            "Image export 05WNU_2016_26 has started\n",
            "Image export 05WNU_2016_27 has started\n",
            "Image export 05WNU_2016_28 has started\n",
            "Image export 05WNU_2016_29 has started\n",
            "Image export 05WNU_2016_30 has started\n",
            "Image export 05WNU_2016_31 has started\n",
            "Image export 05WNU_2016_32 has started\n",
            "Image export 05WNU_2016_33 has started\n",
            "Image export 05WNU_2016_34 has started\n",
            "Image export 05WNU_2016_35 has started\n",
            "Image export 05WNU_2016_36 has started\n",
            "Image export 05WNU_2016_37 has started\n",
            "Image export 05WNU_2016_38 has started\n",
            "Image export 05WNU_2016_39 has started\n"
          ]
        }
      ],
      "source": [
        "for i, id in enumerate(ids):\n",
        "  for d1 in dates_list:\n",
        "     # d1 = ee.Date(start).format('YYYY-MM-dd')\n",
        "      d1 = d1['value']\n",
        "      start = ee.Date(d1);\n",
        "      end = ee.Date(d1).advance(1, 'week');\n",
        "      date_range = ee.DateRange(start, end);\n",
        "      eeroi = tiles.filter(ee.Filter.eq('Name', id)).first() # Filter grid to tile to define roi\n",
        "      roi = ee.Geometry.Polygon(eeroi.geometry().getInfo()['coordinates'][0]) # define roi as geometry variable\n",
        "\n",
        "       #### EXPORT DESCRIPTION\n",
        "      year = start.get('year').getInfo();\n",
        "      month = start.get('month').getInfo();\n",
        "      week  = start.get('week').getInfo();\n",
        "\n",
        "      description = str(id) + \"_\" + str(year) + \"_\" + str(week) # Export Name\n",
        "      ###############################################################################\n",
        "      ## ***IMAGE PRE-PROCESSING METHODS***\n",
        "      ###############################################################################\n",
        "\n",
        "      # get images and cloud probability. help from: https://developers.google.com/earth-engine/tutorials/community/sentinel-2-s2cloudless\n",
        "      def get_s2_sr_cld_col(aoi):\n",
        "          # Import and filter S2 SR.\n",
        "          s2_sr_col = ee.ImageCollection('COPERNICUS/S2_HARMONIZED').filterBounds(aoi).filterDate(start,end).filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',50))\n",
        "          # Import and filter s2cloudless\n",
        "          s2_cloudless_col = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY').filterBounds(aoi).filterDate(start,end)\n",
        "          # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
        "          return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
        "              'primary': s2_sr_col,\n",
        "              'secondary': s2_cloudless_col,\n",
        "              'condition': ee.Filter.equals( # Pass leftField and rightField as separate arguments\n",
        "                  leftField= 'system:index',\n",
        "                  rightField= 'system:index'\n",
        "                  )\n",
        "              }))\n",
        "\n",
        "      def add_cloud_bands(img):\n",
        "          # Get s2cloudless image, subset the probability band.\n",
        "          cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
        "\n",
        "          # Condition s2cloudless by the probability threshold value.\n",
        "          is_cloud = cld_prb.gt(50).rename('clouds')\n",
        "\n",
        "          # Add the cloud probability layer and cloud mask as image bands.\n",
        "          return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
        "\n",
        "\n",
        "      def clip2lakes(image):\n",
        "        '''Clips an image based on the lake boundaries'''\n",
        "        return image.clipToCollection(lakes)\n",
        "\n",
        "      # Get percentile cover - saved as metadata\n",
        "\n",
        "\n",
        "      # Mosaic images by date, orbit, - basically combines images together that were taken on the same day\n",
        "      def mosaicBy(imcol):\n",
        "        '''Takes an image collection (imcol) and creates a mosaic for each day\n",
        "        Returns: An image collection of daily mosaics'''\n",
        "        #return the collection as a list of images (not an image collection)\n",
        "        imlist = imcol.toList(imcol.size())\n",
        "        # Get all the dates as list\n",
        "        def imdate(im):\n",
        "          date = ee.Image(im).date().format(\"YYYY-MM-dd\")\n",
        "          return date\n",
        "        all_dates = imlist.map(imdate)\n",
        "        # get all orbits as list\n",
        "        def orbitId(im):\n",
        "          orb = ee.Image(im).get('SENSING_ORBIT_NUMBER')\n",
        "          return orb\n",
        "        all_orbits = imlist.map(orbitId)\n",
        "        # get all spacecraft names as list\n",
        "        def spacecraft(im):\n",
        "          return ee.Image(im).get('SPACECRAFT_NAME')\n",
        "        all_spNames = imlist.map(spacecraft)\n",
        "        # this puts dates, orbits and names into a nested list\n",
        "        concat_all = all_dates.zip(all_orbits).zip(all_spNames);\n",
        "        # here we unnest the list with flatten, and then concatenate the list elements with \" \"\n",
        "        def concat(el):\n",
        "          return ee.List(el).flatten().join(\" \")\n",
        "        concat_all = concat_all.map(concat)\n",
        "        # here, just get distinct combintations of date, orbit and name\n",
        "        concat_unique = concat_all.distinct()\n",
        "        # mosaic\n",
        "        def mosaicIms(d):\n",
        "          d1 = ee.String(d).split(\" \")\n",
        "          date1 = ee.Date(d1.get(0))\n",
        "          orbit = ee.Number.parse(d1.get(1)).toInt()\n",
        "          spName = ee.String(d1.get(2))\n",
        "          im = imcol.filterDate(date1, date1.advance(1, \"day\")).filterMetadata('SPACECRAFT_NAME', 'equals', spName).filterMetadata('SENSING_ORBIT_NUMBER','equals', orbit).mosaic()\n",
        "          return im.set(\n",
        "              \"system:time_start\", date1.millis(),\n",
        "              \"system:date\", date1.format(\"YYYY-MM-dd\"),\n",
        "              \"system:id\", d1)\n",
        "        mosaic_imlist = concat_unique.map(mosaicIms)\n",
        "        return ee.ImageCollection(mosaic_imlist)\n",
        "\n",
        "      # Apply cloud mask to other bands\n",
        "      def applyMask(image):\n",
        "        img = image.updateMask(image.select('clouds').neq(1))\n",
        "        ice = image.select('B4').gte(1050)\n",
        "        maskedice = img.mask(ice.Not())\n",
        "        return maskedice\n",
        "\n",
        "      ###########################################################################\n",
        "      ## ***WATER CLASSIFICATION METHODS***\n",
        "      ###############################################################################\n",
        "\n",
        "      # Define NDWI image\n",
        "      def ndwi(image):\n",
        "        '''Adds an NDWI band to the input image'''\n",
        "        return image.normalizedDifference(['B3', 'B8']).rename('NDWI').multiply(1000)\n",
        "\n",
        "      # Basic ndwi classification\n",
        "      def ndwi_classify(image):\n",
        "        '''Creates a binary image based on an NDWI threshold of 0'''\n",
        "        ndwimask = image.select('NDWI')\n",
        "        water = ndwimask.gte(0)\n",
        "        land = ndwimask.lt(0)\n",
        "        return(water)\n",
        "\n",
        "      # OTSU thresholding from histogram\n",
        "      def otsu(histogram):\n",
        "        '''Returns the NDWI threshold for binary water classification'''\n",
        "        counts = ee.Array(ee.Dictionary(histogram).get('histogram'))\n",
        "        means = ee.Array(ee.Dictionary(histogram).get('bucketMeans'))\n",
        "        size = means.length().get([0])\n",
        "        total = counts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
        "        sum = means.multiply(counts).reduce(ee.Reducer.sum(), [0]).get([0])\n",
        "        mean = sum.divide(total)\n",
        "        indices = ee.List.sequence(1, size)\n",
        "        def func_xxx(i):\n",
        "          '''Compute between sum of squares, where each mean partitions the data.'''\n",
        "          aCounts = counts.slice(0, 0, i)\n",
        "          aCount = aCounts.reduce(ee.Reducer.sum(), [0]).get([0])\n",
        "          aMeans = means.slice(0, 0, i)\n",
        "          aMean = aMeans.multiply(aCounts) \\\n",
        "              .reduce(ee.Reducer.sum(), [0]).get([0]) \\\n",
        "              .divide(aCount)\n",
        "          bCount = total.subtract(aCount)\n",
        "          bMean = sum.subtract(aCount.multiply(aMean)).divide(bCount)\n",
        "          return aCount.multiply(aMean.subtract(mean).pow(2)).add(\n",
        "                bCount.multiply(bMean.subtract(mean).pow(2)))\n",
        "        bss = indices.map(func_xxx)\n",
        "        # Return the mean value corresponding to the maximum BSS.\n",
        "        return means.sort(bss).get([-1])\n",
        "\n",
        "      # OTSU thresholding for an image\n",
        "      def otsu_thresh(water_image):\n",
        "        '''Calculate NDWI and create histogram. Return the OTSU threshold.'''\n",
        "        NDWI = ndwi(water_image).select('NDWI').updateMask(water_image.select('clouds').neq(1))\n",
        "        histogram = ee.Dictionary(NDWI.reduceRegion(\n",
        "          geometry = roi,\n",
        "          reducer = ee.Reducer.histogram(255, 2).combine('mean', None, True).combine('variance', None, True),\n",
        "          scale = pixScale,\n",
        "          maxPixels = 1e12\n",
        "        ))\n",
        "        return otsu(histogram.get('NDWI_histogram'))\n",
        "\n",
        "      # Classify an image using OTSU threshold.\n",
        "      def otsu_classify(water_image):\n",
        "        '''(1) Calculate NDWI and create histogram. (2) Calculate NDWI threshold for\n",
        "        binary classification using OTSU method. (3) Classify image and add layer to input image.\n",
        "        '''\n",
        "        NDWI = ndwi(water_image).select('NDWI')\n",
        "        histogram = ee.Dictionary(NDWI.reduceRegion(\n",
        "          geometry = roi,\n",
        "          reducer = ee.Reducer.histogram(255, 2).combine('mean', None, True).combine('variance', None, True),\n",
        "          scale = pixScale,\n",
        "          maxPixels = 1e12\n",
        "        ))\n",
        "        threshold = otsu(histogram.get('NDWI_histogram'))\n",
        "        otsu_classed = NDWI.gt(ee.Number(threshold)).And(water_image.select('B8').lt(2000)).rename('otsu_classed')\n",
        "        return water_image.addBands([otsu_classed])\n",
        "\n",
        "      def adaptive_thresholding(water_image):\n",
        "        '''Takes an image clipped to lakes and returns the water mask'''\n",
        "        NDWI = ndwi(water_image).select('NDWI')\n",
        "        threshold = ee.Number(otsu_thresh(water_image))\n",
        "        threshold = threshold.divide(10).round().multiply(10)\n",
        "        # get fixed histogram\n",
        "        histo = NDWI.reduceRegion(\n",
        "            geometry = roi,\n",
        "            reducer = ee.Reducer.fixedHistogram(-1000, 1000, 200),\n",
        "            scale = pixScale, # This was 30, keep at 10!?!?\n",
        "            maxPixels = 1e12\n",
        "        )\n",
        "        hist = ee.Array(histo.get('NDWI'))\n",
        "        counts = hist.cut([-1,1])\n",
        "        buckets = hist.cut([-1,0])\n",
        "        #find split points from otsu threshold\n",
        "        threshold = ee.Array([threshold]).toList()\n",
        "        buckets_list = buckets.toList()\n",
        "        split = buckets_list.indexOf(threshold)\n",
        "        # split into land and water slices\n",
        "        land_slice = counts.slice(0,0,split)\n",
        "        water_slice = counts.slice(0,split.add(1),-1)\n",
        "        # find max of land and water slices\n",
        "        land_max = land_slice.reduce(ee.Reducer.max(),[0])\n",
        "        water_max = water_slice.reduce(ee.Reducer.max(),[0])\n",
        "        land_max = land_max.toList().get(0)\n",
        "        water_max = water_max.toList().get(0)\n",
        "        land_max = ee.List(land_max).getNumber(0)\n",
        "        water_max = ee.List(water_max).getNumber(0)\n",
        "        #find difference between land, water and otsu val\n",
        "        counts_list = counts.toList()\n",
        "        otsu_val = ee.Number(counts_list.get(split))\n",
        "        otsu_val = ee.List(otsu_val).getNumber(0)\n",
        "        land_prom = ee.Number(land_max).subtract(otsu_val)\n",
        "        water_prom = ee.Number(water_max).subtract(otsu_val)\n",
        "        #find land and water buckets corresponding to 0.9 times the prominence\n",
        "        land_thresh = ee.Number(land_max).subtract((land_prom).multiply(ee.Number(0.9)))\n",
        "        water_thresh = ee.Number(water_max).subtract((water_prom).multiply(ee.Number(0.9)))\n",
        "        land_max_ind = land_slice.argmax().get(0)\n",
        "        water_max_ind = water_slice.argmax().get(0)\n",
        "        li = ee.Number(land_max_ind).subtract(1)\n",
        "        li = li.max(ee.Number(1))\n",
        "        wi = ee.Number(water_max_ind).add(1)\n",
        "        wi = wi.min(ee.Number(199))\n",
        "        land_slice2 = land_slice.slice(0,li,-1).subtract(land_thresh)\n",
        "        water_slice2 = water_slice.slice(0,0,wi).subtract(water_thresh)\n",
        "        land_slice2  = land_slice2.abs().multiply(-1)\n",
        "        water_slice2 = water_slice2.abs().multiply(-1)\n",
        "        land_index = ee.Number(land_slice2.argmax().get(0)).add(land_max_ind)\n",
        "        water_index = ee.Number(water_slice2.argmax().get(0)).add(split)\n",
        "        land_level = ee.Number(buckets_list.get(land_index))\n",
        "        water_level = ee.Number(buckets_list.get(water_index))\n",
        "        land_level = ee.Number(ee.List(land_level).get(0)).add(5)\n",
        "        water_level = ee.Number(ee.List(water_level).get(0)).add(5)\n",
        "        #calculate water fraction and classify\n",
        "        water_fraction = (NDWI.subtract(land_level)).divide(water_level.subtract(land_level)).multiply(100).rename('water_fraction')\n",
        "        #water_fraction = conditional(water_fraction) #sets values less than 0 to 0 and greater than 100 to 100\n",
        "        water_75 = water_fraction.gte(75).rename('water_75'); #note, this is a non-binary classification, so we use 75% water as \"water\"\n",
        "        all_mask = water_image.select('B2').gt(5).rename('all_mask')\n",
        "        cloud_mask_ed = water_image.select('clouds').neq(0).rename('cloud_mask_ed')\n",
        "        return water_image.addBands([water_fraction,water_75,NDWI,cloud_mask_ed])\n",
        "\n",
        "      def binaryImage(image):\n",
        "        '''takes a multiband image and returns just the binary water_75 band'''\n",
        "        img = image.select('water_75').rename('water_occurance')\n",
        "        return img\n",
        "      def waterImage(image):\n",
        "        '''takes a multiband image and returns just the water fraction band'''\n",
        "        img = image.select('water_fraction')\n",
        "        return img\n",
        "\n",
        "\n",
        "\n",
        "      ## ***Lake Occurrence Raster***\n",
        "      # (1) image pre-process:\n",
        "\n",
        "      # Get images and cloud probability, add cloud bands\n",
        "      images = get_s2_sr_cld_col(roi).map(add_cloud_bands)\n",
        "     # if images.size().getInfo() == 0:\n",
        "      #  print('No S2 images for ' + description)\n",
        "       # continue\n",
        "     # print(images.size().getInfo())\n",
        "      # Mosaic images and clip to ROI\n",
        "      # Clip image\n",
        "      def clip_image(image):\n",
        "          return image.clip(roi)\n",
        "\n",
        "      images_all = mosaicBy(images).map(clip_image)\n",
        "      # Get percent cover for each mosaic\n",
        "      image_mask = images_all.select('B2').mean().clip(roi).gte(0)  #First, calculate total number of pixels\n",
        "      totPixels = ee.Number(image_mask.reduceRegion(\n",
        "          reducer = ee.Reducer.count(),\n",
        "          scale = 100,\n",
        "          geometry = roi,\n",
        "          maxPixels = 1e12\n",
        "          ).values().get(0))\n",
        "\n",
        "      def getCover(image):\n",
        "        actPixels = ee.Number(image.updateMask(image.select('clouds').neq(1)).reduceRegion(\n",
        "                      reducer = ee.Reducer.count(),\n",
        "                      scale = 100,\n",
        "                      geometry = roi,\n",
        "                      maxPixels=1e12,\n",
        "                      ).values().get(0))\n",
        "        percCover = actPixels.divide(totPixels).multiply(100).round()\n",
        "        ice = image.select('B4').gte(1050)\n",
        "        maskedice = image.mask(ice.Not())\n",
        "        icePixels = ee.Number(maskedice\n",
        "                .reduceRegion(\n",
        "                      reducer = ee.Reducer.count(),\n",
        "                      scale = 100,\n",
        "                      geometry = roi,\n",
        "                      maxPixels = 1e12,\n",
        "                      ).values().get(0))\n",
        "        percIce = icePixels.divide(totPixels).multiply(100).round()\n",
        "      # number as output\n",
        "        return image.set('cloud_free_cover', percCover,'actPixels',actPixels, 'ice_free_cover', percIce)\n",
        "\n",
        "      coverimages = images_all.map(getCover)\n",
        "      # Filter by percent cover, remove images with the mask covering more than 30% of the ROI)\n",
        "      cleancoverimages = coverimages.filterMetadata('cloud_free_cover','greater_than',70).filterMetadata('ice_free_cover','greater_than',10)\n",
        "\n",
        "     # if cleancoverimages.size().getInfo() == 0:\n",
        "      #  print(\"No images meet the cloud and ice cover criteria for \" + description)\n",
        "       # continue\n",
        "\n",
        "      # Apply cloud mask to other bands\n",
        "      cloudmaskedimgs = cleancoverimages.map(applyMask)\n",
        "      # (2) water classification:\n",
        "\n",
        "      # adaptive thresholding on every lake image\n",
        "      thesholdedimages = cloudmaskedimgs.map(adaptive_thresholding)\n",
        "      # create image collection of just binary water images\n",
        "      waterimages = thesholdedimages.map(binaryImage)\n",
        "\n",
        "      #clip to just include lakes\n",
        "      lakeimages = waterimages.map(clip2lakes)#.filter(ee.Filter.listContains(\"system:band_names\", \"water_occurance\"))\n",
        "      # get max of images\n",
        "      finalimage = lakeimages.max()\n",
        "\n",
        "      task = ee.batch.Export.image.toAsset(**{\n",
        "              'image': ee.Image(finalimage),\n",
        "              'description': description,\n",
        "              'assetId': 'projects/alpod-412314/assets/' + regionfolder  + description,\n",
        "              'scale': 10,\n",
        "              'region': roi,\n",
        "              'maxPixels': 1e12,\n",
        "              })\n",
        "      task.start()\n",
        "      print(\"Image export \" +  description +  \" has started\")\n",
        "     # else:\n",
        "      #  print(\"No images for \" + description)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
